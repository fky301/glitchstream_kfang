{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251c44e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8192)\n",
      "['Blip' 'Koi_Fish' 'Scattered_Light' 'Tomte']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ghat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.6649329863064288, -0.013300431395350643, -...</td>\n",
       "      <td>Tomte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.5218259913611227, 2.1265724069141783, 1.36...</td>\n",
       "      <td>Scattered_Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0539129622272867, -0.062193694494517615, 3....</td>\n",
       "      <td>Blip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2.8798052331645216, 1.2827490522864409, 0.380...</td>\n",
       "      <td>Tomte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3.6875115093197017, -0.8832482481496595, 3.89...</td>\n",
       "      <td>Scattered_Light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ghat            label\n",
       "0  [-0.6649329863064288, -0.013300431395350643, -...            Tomte\n",
       "1  [-1.5218259913611227, 2.1265724069141783, 1.36...  Scattered_Light\n",
       "2  [1.0539129622272867, -0.062193694494517615, 3....             Blip\n",
       "3  [2.8798052331645216, 1.2827490522864409, 0.380...            Tomte\n",
       "4  [3.6875115093197017, -0.8832482481496595, 3.89...  Scattered_Light"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and metadata from saved files, combine them, and display unique labels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from glitchstream.glitch_downloader import GlitchDownloader\n",
    "# from glitchstream.deepextractor import DeepExtractor\n",
    "\n",
    "base_dir = pathlib.Path('./sample_glitches')\n",
    "\n",
    "data_subs = []\n",
    "for i in range(1):\n",
    "\tdata_subs.append(np.load(base_dir/f'random_glitches_{i}_g_hats.npy',allow_pickle=True))\n",
    "data = np.concatenate(data_subs,axis=0)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "metadatas = []\n",
    "for i in range(1):\n",
    "\tmetadatas.append(pd.read_csv(base_dir/f'random_glitches_{i}_metadataframe.csv'))\n",
    "metadata_df = pd.DataFrame(pd.concat(metadatas,ignore_index=True))\n",
    "\n",
    "ghat_labels = metadata_df['ml_label'].to_numpy()\n",
    "ghat_labels_uq = np.unique(ghat_labels)\n",
    "print(np.unique(metadata_df['ml_label'].to_numpy()))\n",
    "\n",
    "ghat_df = pd.DataFrame({'ghat':list(data),'label':ghat_labels})\n",
    "\n",
    "ghat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# half-supervised kshape clustering, centroid initialization consist of known labels + randomly selected samples\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.metrics import cdist_normalized_cc\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df_train, df_test = train_test_split(ghat_df, train_size=0.8, random_state=42,stratify=ghat_df['label'],shuffle=True)\n",
    "\n",
    "ghat_train = np.array(df_train.ghat.to_list())\n",
    "label_train = np.array(df_train.label.to_list())\n",
    "ghat_test = np.array(df_test.ghat.to_list())\n",
    "label_test = np.array(df_test.label.to_list())\n",
    "\n",
    "\n",
    "X_train_ts = to_time_series_dataset(ghat_train)\n",
    "X_test_ts = to_time_series_dataset(ghat_test)\n",
    "\n",
    "# resample timeseries to smaller size for faster computation\n",
    "sz_resample = 2048\n",
    "X_train_ts_resampled = TimeSeriesResampler(sz=sz_resample).fit_transform(X_train_ts)\n",
    "X_train_ts_scaled = TimeSeriesScalerMeanVariance().fit_transform(X_train_ts_resampled)\n",
    "\n",
    "k_range = range(3,10) # number of clusters to try 3,4,5,6,7,8,9\n",
    "inertia_list = []\n",
    "ARI_list = []\n",
    "\n",
    "known_centroids = []\n",
    "\n",
    "ks_half_supervised_list = []\n",
    "y_pred_ks_half_supervised_list = []\n",
    "init_centroids_list = []\n",
    "\n",
    "for i in range(len(ghat_labels_uq)):\n",
    "    X_subset = X_train_ts_scaled[label_train == ghat_labels_uq[i]]\n",
    "    ks_subset = KShape(n_clusters=1,random_state=42,init='random')\n",
    "    ks_subset.fit(X_subset)\n",
    "    known_centroids.append(ks_subset.cluster_centers_[0]) # get centroid for each known labeled class\n",
    "\n",
    "# define a function to pick random samples as initial centroids for unknown clusters, we need to make sure their distances from\n",
    "# known centroids are large enough than certain threshold (to avoid overlapping clusters)\n",
    "def sbd_distance_matrix(X,Y):\n",
    "    X_norms = np.linalg.norm(X,axis=(1,2))\n",
    "    Y_norms = np.linalg.norm(Y,axis=(1,2))\n",
    "    ncc_matrix = cdist_normalized_cc(X,Y,norms1=X_norms,norms2=Y_norms,self_similarity=False)\n",
    "    return 1. - ncc_matrix\n",
    "\n",
    "def find_safe_random_sample(X,known_centroids,random_centroids,threshold=0.75):  # input: timeseries dataset, known centroids list, random centroids list, dissimilarity threshold\n",
    "    max_tries = 1000\n",
    "    if len(random_centroids) > 0:\n",
    "        known_centroids = known_centroids + random_centroids # exclude both known and already selected random centroids\n",
    "    for _ in range(max_tries):\n",
    "        rand_idx = np.random.randint(X.shape[0])\n",
    "        candidate = X[rand_idx].reshape(1,-1,1)\n",
    "        dists = sbd_distance_matrix(candidate,np.array(known_centroids))\n",
    "        if np.min(dists**2) >= threshold:\n",
    "            new_centroid = candidate.reshape(sz_resample,1)\n",
    "            return new_centroid\n",
    "    raise ValueError(\"Could not find a safe random sample after maximum tries\") # if not found within max tries, raise error and abort\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k in k_range:\n",
    "    if k < len(ghat_labels_uq):\n",
    "        ks_half_supervised = KShape(n_clusters=k,random_state=42,init='random',n_init=10) # if k less than known clusters, random initialize\n",
    "        init_centroids = 'random_initialization'\n",
    "    else:\n",
    "        n_random = k - len(ghat_labels_uq) # number of unknown clusters to add\n",
    "        random_centroids = []\n",
    "        for j in range(n_random): # begin from known centroids, add random centroids one by one\n",
    "            rand_cent = find_safe_random_sample(X_train_ts_scaled,known_centroids,random_centroids)\n",
    "            random_centroids.append(rand_cent)\n",
    "        init_centroids = np.array(known_centroids + random_centroids)\n",
    "        ks_half_supervised = KShape(n_clusters=k,random_state=42,init=init_centroids)\n",
    "    y_pred_ks_half_supervised = ks_half_supervised.fit_predict(X_train_ts_scaled)\n",
    "    ks_half_supervised_list.append(ks_half_supervised)\n",
    "    y_pred_ks_half_supervised_list.append(y_pred_ks_half_supervised)\n",
    "    init_centroids_list.append(init_centroids)\n",
    "    inertia_list.append(ks_half_supervised.inertia_)\n",
    "    ARI_list.append(adjusted_rand_score(label_train,y_pred_ks_half_supervised)) # compare predicted labels with known labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inertia and ARI vs number of clusters k for evaluation of clustering performance\n",
    "\n",
    "fig, ax1 = plt.figure(figsize=(8,6)), plt.gca()\n",
    "\n",
    "ax1.set_xlabel('Number of clusters k')\n",
    "ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "ax1.plot(k_range, inertia_list, marker='o', color='tab:blue', label='Inertia')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Adjusted Rand Index (ARI)', color='tab:red')\n",
    "ax2.plot(k_range, ARI_list, marker='s', color='tab:red', label='ARI')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Inertia and Adjusted Rand Index (ARI) vs Number of Clusters k')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing function for visualizing cluster centroids and collection of time series in each cluster\n",
    "def plot_clusters(ks_model, X_ts, y_pred, k):\n",
    "    n_clusters = ks_model.n_clusters\n",
    "    fig, axes = plt.subplots(n_clusters, 1, figsize=(12, 3 * n_clusters))\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        ax = axes[cluster_idx]\n",
    "        cluster_members = X_ts[y_pred == cluster_idx]\n",
    "        for ts in cluster_members:\n",
    "            ax.plot(ts.ravel(), color='gray', alpha=0.2)\n",
    "        centroid = ks_model.cluster_centers_[cluster_idx]\n",
    "        ax.plot(centroid.ravel(), color='red', linewidth=2)\n",
    "        ax.set_title(f'Cluster {cluster_idx + 1} (k={k}) - Size: {cluster_members.shape[0]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw cluster centeroids for kbest from elbow method and ARI\n",
    "\n",
    "kbest = 7 # kbest based on elbow method and ARI\n",
    "ks_best = ks_half_supervised_list[kbest - min(k_range)]\n",
    "y_pred_ks_best = y_pred_ks_half_supervised_list[kbest - min(k_range)]\n",
    "label_ks_best = np.unique(y_pred_ks_best)\n",
    "\n",
    "plot_clusters(ks_best, X_train_ts_scaled, y_pred_ks_best, kbest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gwsimenv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
